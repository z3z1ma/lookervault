"""Edge case tests for storage module.

Tests cover:
- Database connection failures
- Concurrent writes to database
- Malformed data in serialization/deserialization
- Invalid timestamps
- Large content data
- Empty/None values
- SQLite busy handling
"""

import sqlite3
import threading
import time
from datetime import UTC, datetime

import pytest

from lookervault.exceptions import StorageError
from lookervault.storage.models import (
    Checkpoint,
    ContentItem,
    ContentType,
    DeadLetterItem,
    ExtractionSession,
    SessionStatus,
)
from lookervault.storage.repository import SQLiteContentRepository
from lookervault.storage.serializer import MsgpackSerializer


class TestDatabaseConnectionFailures:
    """Tests for handling database connection failures."""

    def test_repository_handles_corrupt_database(self, tmp_path):
        """Test repository handles corrupted database gracefully."""
        db_path = tmp_path / "corrupt.db"

        # Create invalid database file
        db_path.write_bytes(b"This is not a valid SQLite database")

        with pytest.raises((StorageError, sqlite3.DatabaseError)):
            SQLiteContentRepository(db_path)

    def test_repository_creates_new_database(self, tmp_path):
        """Test repository creates new database if missing."""
        db_path = tmp_path / "new.db"

        # Should create database
        repo = SQLiteContentRepository(db_path)
        assert db_path.exists()

        # Verify we can query schema version
        version = repo.get_schema_version()
        assert version > 0

    def test_repository_readonly_database(self, tmp_path):
        """Test repository handles read-only database."""
        db_path = tmp_path / "readonly.db"

        # Create database
        repo = SQLiteContentRepository(db_path)
        item = ContentItem(
            id="test",
            content_type=ContentType.DASHBOARD.value,
            name="Test",
            owner_id=1,
            owner_email="test@example.com",
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC),
            synced_at=datetime.now(UTC),
            content_data=b"data",
        )
        repo.save_content(item)
        repo.close()

        # Make file read-only
        db_path.chmod(0o444)

        # Read operations should work
        repo2 = SQLiteContentRepository(db_path)
        retrieved = repo2.get_content("test")
        assert retrieved is not None

        # Write operations should fail
        with pytest.raises(StorageError):
            repo2.save_content(item)

    def test_thread_connection_isolation(self, tmp_path):
        """Test each thread gets its own connection."""
        repo = SQLiteContentRepository(tmp_path / "test.db")

        connections = []

        def get_conn():
            conn = repo._get_connection()
            connections.append(id(conn))
            repo.close_thread_connection()

        threads = [threading.Thread(target=get_conn) for _ in range(5)]

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # All connections should have different IDs
        assert len(set(connections)) == 5


class TestConcurrentWrites:
    """Tests for concurrent database write operations."""

    def test_concurrent_save_content(self, tmp_path):
        """Test concurrent save_content operations."""
        repo = SQLiteContentRepository(tmp_path / "concurrent.db")

        num_threads = 10
        items_per_thread = 50
        errors = []

        def save_items(thread_id):
            try:
                for i in range(items_per_thread):
                    item = ContentItem(
                        id=f"thread_{thread_id}_item_{i}",
                        content_type=ContentType.DASHBOARD.value,
                        name=f"Item {i}",
                        owner_id=thread_id,
                        owner_email=f"thread{thread_id}@example.com",
                        created_at=datetime.now(UTC),
                        updated_at=datetime.now(UTC),
                        synced_at=datetime.now(UTC),
                        content_data=b"data",
                    )
                    repo.save_content(item)
            except Exception as e:
                errors.append((thread_id, e))
            finally:
                repo.close_thread_connection()

        threads = [threading.Thread(target=save_items, args=(i,)) for i in range(num_threads)]

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # Should have no errors
        assert len(errors) == 0

        # Verify all items saved
        count = repo.count_content(ContentType.DASHBOARD.value)
        assert count == num_threads * items_per_thread

    def test_concurrent_checkpoint_saves(self, tmp_path):
        """Test concurrent checkpoint save operations."""
        repo = SQLiteContentRepository(tmp_path / "checkpoint.db")

        num_threads = 5
        checkpoints_per_thread = 10
        errors = []

        def save_checkpoints(thread_id):
            try:
                for i in range(checkpoints_per_thread):
                    checkpoint = Checkpoint(
                        session_id=f"session_{thread_id}",
                        content_type=ContentType.DASHBOARD.value,
                        checkpoint_data={"thread": thread_id, "index": i},
                        started_at=datetime.now(UTC),
                        item_count=i,
                    )
                    repo.save_checkpoint(checkpoint)
            except Exception as e:
                errors.append((thread_id, e))
            finally:
                repo.close_thread_connection()

        threads = [threading.Thread(target=save_checkpoints, args=(i,)) for i in range(num_threads)]

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        assert len(errors) == 0

    def test_busy_retry_logic(self, tmp_path):
        """Test that busy retry logic works."""
        repo = SQLiteContentRepository(tmp_path / "busy.db")

        # Simulate concurrent writes to same content
        num_writes = 20
        errors = []

        def write_same_item(thread_id):
            try:
                for i in range(num_writes):
                    item = ContentItem(
                        id="same_id",
                        content_type=ContentType.DASHBOARD.value,
                        name=f"Update {i}",
                        owner_id=1,
                        owner_email="test@example.com",
                        created_at=datetime.now(UTC),
                        updated_at=datetime.now(UTC),
                        synced_at=datetime.now(UTC),
                        content_data=b"data",
                    )
                    repo.save_content(item)
                    time.sleep(0.01)  # Small delay to increase contention
            except Exception as e:
                errors.append((thread_id, e))
            finally:
                repo.close_thread_connection()

        threads = [threading.Thread(target=write_same_item, args=(i,)) for i in range(5)]

        for t in threads:
            t.start()
        for t in threads:
            t.join()

        # Retry logic should handle busy errors
        assert len(errors) == 0

        # Verify final state
        retrieved = repo.get_content("same_id")
        assert retrieved is not None


class TestSerializationEdgeCases:
    """Tests for serialization edge cases."""

    def test_serialize_empty_dict(self):
        """Test serializing empty dictionary."""
        serializer = MsgpackSerializer()

        serialized = serializer.serialize({})
        deserialized = serializer.deserialize(serialized)

        assert deserialized == {}

    def test_serialize_empty_list(self):
        """Test serializing empty list."""
        serializer = MsgpackSerializer()

        serialized = serializer.serialize([])
        deserialized = serializer.deserialize(serialized)

        assert deserialized == []

    def test_serialize_large_data(self):
        """Test serializing large data structures."""
        serializer = MsgpackSerializer()

        # Create large nested structure
        large_data = {
            "items": [{"id": i, "data": "x" * 100} for i in range(1000)],
            "metadata": {"key": "value" * 100},
        }

        serialized = serializer.serialize(large_data)
        deserialized = serializer.deserialize(serialized)

        assert deserialized == large_data

    def test_serialize_deeply_nested(self):
        """Test serializing deeply nested structures."""
        serializer = MsgpackSerializer()

        # Create deeply nested structure
        nested = {"level": 0}
        current = nested
        for i in range(1, 50):
            current["next"] = {"level": i}
            current = current["next"]

        serialized = serializer.serialize(nested)
        deserialized = serializer.deserialize(serialized)

        assert isinstance(deserialized, dict)
        assert deserialized["level"] == 0
        assert isinstance(deserialized["next"], dict)
        assert deserialized["next"]["level"] == 1

    def test_validate_invalid_blob(self):
        """Test validate rejects invalid blobs."""
        serializer = MsgpackSerializer()

        # Invalid msgpack data
        assert not serializer.validate(b"invalid msgpack data")
        assert not serializer.validate(b"\x00\x01\x02\x03")

    def test_serialize_with_binary_data(self):
        """Test serializing data with binary content."""
        serializer = MsgpackSerializer()

        data = {
            "id": "test",
            "binary": b"\x00\x01\x02\xff",
            "mixed": [1, "text", b"\xab\xcd"],
        }

        serialized = serializer.serialize(data)
        deserialized = serializer.deserialize(serialized)

        assert isinstance(deserialized, dict)
        assert deserialized["id"] == "test"
        assert deserialized["binary"] == data["binary"]
        assert deserialized["mixed"] == data["mixed"]


class TestInvalidTimestamps:
    """Tests for handling invalid timestamps."""

    def test_content_item_with_invalid_timestamp(self, tmp_path):
        """Test handling of invalid timestamp strings."""
        repo = SQLiteContentRepository(tmp_path / "timestamps.db")

        # SQLite accepts invalid timestamp strings as-is, validation happens at application layer
        # Let's verify the timestamp is stored and retrieved as-is
        item_dict = {
            "id": "test",
            "content_type": ContentType.DASHBOARD.value,
            "name": "Test",
            "owner_id": 1,
            "created_at": "not-a-valid-timestamp",
            "updated_at": "2024-01-01T00:00:00Z",
            "synced_at": "2024-01-01T00:00:00Z",
            "content_data": b"data",
            "content_size": 4,
        }

        # Manually insert with invalid timestamp
        conn = repo._get_connection()
        cursor = conn.cursor()
        cursor.execute(
            """
            INSERT INTO content_items (id, content_type, name, owner_id,
                                       created_at, updated_at, synced_at, content_data, content_size)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (
                item_dict["id"],
                item_dict["content_type"],
                item_dict["name"],
                item_dict["owner_id"],
                item_dict["created_at"],
                item_dict["updated_at"],
                item_dict["synced_at"],
                item_dict["content_data"],
                item_dict["content_size"],
            ),
        )
        conn.commit()

        # Verify it was stored - retrieval will fail when trying to parse the invalid timestamp
        with pytest.raises((StorageError, ValueError)):
            repo.get_content("test")

    def test_datetime_edge_cases(self, tmp_path):
        """Test datetime boundary values."""
        repo = SQLiteContentRepository(tmp_path / "datetime.db")

        # Test with very old and very new dates
        old_date = datetime(1970, 1, 1, tzinfo=UTC)
        future_date = datetime(2099, 12, 31, 23, 59, 59, tzinfo=UTC)

        item = ContentItem(
            id="test_dates",
            content_type=ContentType.DASHBOARD.value,
            name="Test Dates",
            owner_id=1,
            owner_email="test@example.com",
            created_at=old_date,
            updated_at=future_date,
            synced_at=datetime.now(UTC),
            content_data=b"data",
        )

        repo.save_content(item)

        # Retrieve and verify
        retrieved = repo.get_content("test_dates")
        assert retrieved is not None
        assert retrieved.created_at == old_date
        assert retrieved.updated_at == future_date


class TestLargeContentData:
    """Tests for handling large content data."""

    def test_save_large_content(self, tmp_path):
        """Test saving large content items."""
        repo = SQLiteContentRepository(tmp_path / "large.db")

        # Create content with 1MB of data
        large_data = b"x" * (1024 * 1024)

        item = ContentItem(
            id="large_item",
            content_type=ContentType.DASHBOARD.value,
            name="Large Item",
            owner_id=1,
            owner_email="test@example.com",
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC),
            synced_at=datetime.now(UTC),
            content_data=large_data,
        )

        repo.save_content(item)

        # Retrieve and verify
        retrieved = repo.get_content("large_item")
        assert retrieved is not None
        assert len(retrieved.content_data) == len(large_data)
        assert retrieved.content_size == len(large_data)

    def test_save_multiple_large_items(self, tmp_path):
        """Test saving multiple large items."""
        repo = SQLiteContentRepository(tmp_path / "multiple_large.db")

        num_items = 10
        data_size = 500 * 1024  # 500KB each

        for i in range(num_items):
            large_data = b"y" * data_size
            item = ContentItem(
                id=f"large_{i}",
                content_type=ContentType.DASHBOARD.value,
                name=f"Large Item {i}",
                owner_id=1,
                owner_email="test@example.com",
                created_at=datetime.now(UTC),
                updated_at=datetime.now(UTC),
                synced_at=datetime.now(UTC),
                content_data=large_data,
            )
            repo.save_content(item)

        # Verify all saved
        count = repo.count_content(ContentType.DASHBOARD.value)
        assert count == num_items


class TestEmptyAndNoneValues:
    """Tests for handling empty and None values."""

    def test_save_content_with_none_fields(self, tmp_path):
        """Test saving content with None optional fields."""
        repo = SQLiteContentRepository(tmp_path / "none_fields.db")

        item = ContentItem(
            id="none_test",
            content_type=ContentType.DASHBOARD.value,
            name="None Test",
            owner_id=None,  # None owner
            owner_email=None,  # None email
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC),
            synced_at=datetime.now(UTC),
            deleted_at=None,  # None deleted_at
            folder_id=None,  # None folder_id
            content_data=b"data",
        )

        repo.save_content(item)

        # Retrieve and verify
        retrieved = repo.get_content("none_test")
        assert retrieved is not None
        assert retrieved.owner_id is None
        assert retrieved.owner_email is None
        assert retrieved.deleted_at is None
        assert retrieved.folder_id is None

    def test_save_content_with_empty_strings(self, tmp_path):
        """Test saving content with empty string fields."""
        repo = SQLiteContentRepository(tmp_path / "empty_strings.db")

        item = ContentItem(
            id="empty_test",
            content_type=ContentType.DASHBOARD.value,
            name="",  # Empty name
            owner_id=1,
            owner_email="",  # Empty email
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC),
            synced_at=datetime.now(UTC),
            content_data=b"data",
        )

        repo.save_content(item)

        # Retrieve and verify
        retrieved = repo.get_content("empty_test")
        assert retrieved is not None
        assert retrieved.name == ""
        assert retrieved.owner_email == ""

    def test_update_with_none_values(self, tmp_path):
        """Test updating content with None values."""
        repo = SQLiteContentRepository(tmp_path / "update_none.db")

        # Create with values
        item = ContentItem(
            id="update_test",
            content_type=ContentType.DASHBOARD.value,
            name="Original",
            owner_id=1,
            owner_email="original@example.com",
            created_at=datetime.now(UTC),
            updated_at=datetime.now(UTC),
            synced_at=datetime.now(UTC),
            folder_id="123",
            content_data=b"data",
        )
        repo.save_content(item)

        # Update with None values
        item.name = ""
        item.owner_email = None
        item.folder_id = None
        repo.save_content(item)

        # Retrieve and verify
        retrieved = repo.get_content("update_test")
        assert retrieved is not None
        assert retrieved.name == ""
        assert retrieved.owner_email is None
        assert retrieved.folder_id is None


class TestDeadLetterQueueEdgeCases:
    """Tests for dead letter queue edge cases."""

    def test_dlq_with_long_stack_trace(self, tmp_path):
        """Test DLQ with very long stack trace."""
        repo = SQLiteContentRepository(tmp_path / "dlq_trace.db")

        long_trace = "Traceback (most recent call last):\n" + "  " * 100 + "Error here"

        item = DeadLetterItem(
            session_id="test_session",
            content_id="test_id",
            content_type=ContentType.DASHBOARD.value,
            content_data=b"data",
            error_message="Test error",
            error_type="TestError",
            stack_trace=long_trace,
            retry_count=0,
            failed_at=datetime.now(UTC),
        )

        dlq_id = repo.save_dead_letter_item(item)
        assert dlq_id > 0

        # Retrieve and verify
        retrieved = repo.get_dead_letter_item(dlq_id)
        assert retrieved is not None
        assert retrieved.stack_trace == long_trace

    def test_dlq_with_large_metadata(self, tmp_path):
        """Test DLQ with large metadata."""
        repo = SQLiteContentRepository(tmp_path / "dlq_meta.db")

        large_metadata = {
            "context": {"key": "value" * 1000},
            "history": [{"event": f"Event {i}"} for i in range(100)],
        }

        item = DeadLetterItem(
            session_id="test_session",
            content_id="test_id",
            content_type=ContentType.DASHBOARD.value,
            content_data=b"data",
            error_message="Test error",
            error_type="TestError",
            stack_trace="Traceback here",
            retry_count=0,
            failed_at=datetime.now(UTC),
            metadata=large_metadata,
        )

        dlq_id = repo.save_dead_letter_item(item)

        # Retrieve and verify
        retrieved = repo.get_dead_letter_item(dlq_id)
        assert retrieved is not None
        assert retrieved.metadata == large_metadata

    def test_dlq_with_none_metadata(self, tmp_path):
        """Test DLQ with None metadata."""
        repo = SQLiteContentRepository(tmp_path / "dlq_none_meta.db")

        item = DeadLetterItem(
            session_id="test_session",
            content_id="test_id",
            content_type=ContentType.DASHBOARD.value,
            content_data=b"data",
            error_message="Test error",
            error_type="TestError",
            stack_trace="Traceback here",
            retry_count=0,
            failed_at=datetime.now(UTC),
            metadata=None,
        )

        dlq_id = repo.save_dead_letter_item(item)

        # Retrieve and verify
        retrieved = repo.get_dead_letter_item(dlq_id)
        assert retrieved is not None
        assert retrieved.metadata is None


class TestCheckpointEdgeCases:
    """Tests for checkpoint edge cases."""

    def test_checkpoint_with_empty_data(self, tmp_path):
        """Test checkpoint with empty checkpoint data."""
        repo = SQLiteContentRepository(tmp_path / "empty_checkpoint.db")

        checkpoint = Checkpoint(
            session_id="test_session",
            content_type=ContentType.DASHBOARD.value,
            checkpoint_data={},  # Empty data
            started_at=datetime.now(UTC),
            item_count=0,
        )

        cp_id = repo.save_checkpoint(checkpoint)
        assert cp_id > 0

        # Retrieve and verify
        retrieved = repo.get_latest_checkpoint(ContentType.DASHBOARD.value, "test_session")
        assert retrieved is not None
        assert retrieved.checkpoint_data == {}

    def test_checkpoint_with_complex_data(self, tmp_path):
        """Test checkpoint with complex nested data."""
        repo = SQLiteContentRepository(tmp_path / "complex_checkpoint.db")

        complex_data = {
            "nested": {"key": "value", "numbers": [1, 2, 3]},
            "list": [{"a": 1}, {"b": 2}],
            "unicode": "Test with Ã©mojis ðŸŽ‰",
        }

        checkpoint = Checkpoint(
            session_id="test_session",
            content_type=ContentType.DASHBOARD.value,
            checkpoint_data=complex_data,
            started_at=datetime.now(UTC),
            item_count=100,
        )

        repo.save_checkpoint(checkpoint)

        # Retrieve and verify
        retrieved = repo.get_latest_checkpoint(ContentType.DASHBOARD.value, "test_session")
        assert retrieved is not None
        assert retrieved.checkpoint_data == complex_data


class TestIdMappingEdgeCases:
    """Tests for ID mapping edge cases."""

    def test_id_mapping_with_special_characters(self, tmp_path):
        """Test ID mapping with special characters in IDs."""
        repo = SQLiteContentRepository(tmp_path / "special_ids.db")

        from lookervault.storage.models import IDMapping

        mapping = IDMapping(
            source_instance="https://test.looker.com",
            content_type=ContentType.DASHBOARD.value,
            source_id="id-with/special/chars",  # Special chars
            destination_id="dest/with/slashes",
            created_at=datetime.now(UTC),
            session_id="test_session",
        )

        repo.save_id_mapping(mapping)

        # Retrieve and verify
        retrieved = repo.get_id_mapping(
            "https://test.looker.com",
            ContentType.DASHBOARD.value,
            "id-with/special/chars",
        )
        assert retrieved is not None
        assert retrieved.destination_id == "dest/with/slashes"

    def test_batch_mapping_with_empty_list(self, tmp_path):
        """Test batch mapping with empty source ID list."""
        repo = SQLiteContentRepository(tmp_path / "empty_batch.db")

        result = repo.batch_get_mappings(
            "https://test.looker.com",
            ContentType.DASHBOARD.value,
            [],  # Empty list
        )

        assert result == {}

    def test_batch_mapping_partial_matches(self, tmp_path):
        """Test batch mapping with only some IDs found."""
        repo = SQLiteContentRepository(tmp_path / "partial_batch.db")

        from lookervault.storage.models import IDMapping

        # Save only some mappings
        for i in [1, 3, 5]:
            mapping = IDMapping(
                source_instance="https://test.looker.com",
                content_type=ContentType.DASHBOARD.value,
                source_id=f"source_{i}",
                destination_id=f"dest_{i}",
                created_at=datetime.now(UTC),
                session_id="test_session",
            )
            repo.save_id_mapping(mapping)

        # Query for all IDs (including missing ones)
        result = repo.batch_get_mappings(
            "https://test.looker.com",
            ContentType.DASHBOARD.value,
            ["source_1", "source_2", "source_3", "source_4", "source_5"],
        )

        # Should only return found mappings
        assert len(result) == 3
        assert "source_1" in result
        assert "source_2" not in result
        assert "source_3" in result
        assert "source_4" not in result
        assert "source_5" in result


class TestSessionEdgeCases:
    """Tests for extraction/restoration session edge cases."""

    def test_session_with_empty_metadata(self, tmp_path):
        """Test session with empty metadata."""
        repo = SQLiteContentRepository(tmp_path / "empty_meta.db")

        session = ExtractionSession(
            status=SessionStatus.RUNNING,
            config={"test": "value"},
            metadata={},  # Empty metadata
        )

        repo.create_session(session)

        # Retrieve and verify - empty dict gets stored as empty JSON which returns None when loaded
        retrieved = repo.get_extraction_session(session.id)
        assert retrieved is not None
        # Empty dict is stored as JSON "{}" which sqlite returns, then json.loads returns {}
        # Actually, looking at the code: `json.loads(row["metadata"]) if row["metadata"] else None`
        # So if row["metadata"] is truthy, it parses it
        assert retrieved.metadata is None or retrieved.metadata == {}

    def test_session_with_none_metadata(self, tmp_path):
        """Test session with None metadata."""
        repo = SQLiteContentRepository(tmp_path / "none_meta.db")

        session = ExtractionSession(
            status=SessionStatus.RUNNING,
            config={"test": "value"},
            metadata=None,  # None metadata
        )

        repo.create_session(session)

        # Retrieve and verify
        retrieved = repo.get_extraction_session(session.id)
        assert retrieved is not None
        assert retrieved.metadata is None
